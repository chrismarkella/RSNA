{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Preprocess5\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pympler import asizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../input/rsna-intracranial-hemorrhage-detection/'\n",
    "preprocess = Preprocess5.Preprocess(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dcmFiles = '../../input/rsna-intracranial-hemorrhage-detection/stage_1_train_images/*.dcm'\n",
    "dcm_files = Preprocess5.Preprocess.read_dcm_files(path_dcmFiles)[:N]\n",
    "len(dcm_files)\n",
    "# type(dcm_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = (128, 128)\n",
    "\n",
    "converted_training_images = []\n",
    "# converted_training_dcmData = [] # This will be used for \n",
    "for file_name in dcm_files:\n",
    "    dcm_data = pydicom.dcmread(file_name)\n",
    "    img = preprocess.resizing(dcm_data, desired_size)\n",
    "    converted_training_images.append(img)\n",
    "#     converted_training_dcmData.append(temp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 pixel arrays: 187 MB\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "len(converted_training_images)\n",
    "print(f'{N} pixel arrays: {asizeof.asizeof(converted_training_images)//2**20} MB')\n",
    "print(f'dtype: {converted_training_images[0].dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=plt.figure(figsize=(15, 10))\n",
    "# columns = 5; rows = 6\n",
    "# for i in range(1, columns*rows +1):\n",
    "#     fig.add_subplot(rows, columns, i) \n",
    "#     plt.imshow(converted_training_images[i], cmap=plt.cm.bone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 normalized pixel arrays: 187 MB\n",
      "dtype: float64\n",
      "training_imgs.shape: (500, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "training_imgs = preprocess.transform_all_pixel_arrays(converted_training_images)\n",
    "print(f'{N} normalized pixel arrays: {asizeof.asizeof(training_imgs)//2**20} MB')\n",
    "print(f'dtype: {training_imgs[0].dtype}')\n",
    "training_imgs = np.array(training_imgs)\n",
    "print(f'training_imgs.shape: {training_imgs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally reading in the labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Sub_type</th>\n",
       "      <th>PatientID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361025</th>\n",
       "      <td>ID_000039fa0_any</td>\n",
       "      <td>0</td>\n",
       "      <td>any</td>\n",
       "      <td>000039fa0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361020</th>\n",
       "      <td>ID_000039fa0_epidural</td>\n",
       "      <td>0</td>\n",
       "      <td>epidural</td>\n",
       "      <td>000039fa0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361021</th>\n",
       "      <td>ID_000039fa0_intraparenchymal</td>\n",
       "      <td>0</td>\n",
       "      <td>intraparenchymal</td>\n",
       "      <td>000039fa0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361022</th>\n",
       "      <td>ID_000039fa0_intraventricular</td>\n",
       "      <td>0</td>\n",
       "      <td>intraventricular</td>\n",
       "      <td>000039fa0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361023</th>\n",
       "      <td>ID_000039fa0_subarachnoid</td>\n",
       "      <td>0</td>\n",
       "      <td>subarachnoid</td>\n",
       "      <td>000039fa0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361024</th>\n",
       "      <td>ID_000039fa0_subdural</td>\n",
       "      <td>0</td>\n",
       "      <td>subdural</td>\n",
       "      <td>000039fa0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626117</th>\n",
       "      <td>ID_00005679d_any</td>\n",
       "      <td>0</td>\n",
       "      <td>any</td>\n",
       "      <td>00005679d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626112</th>\n",
       "      <td>ID_00005679d_epidural</td>\n",
       "      <td>0</td>\n",
       "      <td>epidural</td>\n",
       "      <td>00005679d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626113</th>\n",
       "      <td>ID_00005679d_intraparenchymal</td>\n",
       "      <td>0</td>\n",
       "      <td>intraparenchymal</td>\n",
       "      <td>00005679d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626114</th>\n",
       "      <td>ID_00005679d_intraventricular</td>\n",
       "      <td>0</td>\n",
       "      <td>intraventricular</td>\n",
       "      <td>00005679d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626115</th>\n",
       "      <td>ID_00005679d_subarachnoid</td>\n",
       "      <td>0</td>\n",
       "      <td>subarachnoid</td>\n",
       "      <td>00005679d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626116</th>\n",
       "      <td>ID_00005679d_subdural</td>\n",
       "      <td>0</td>\n",
       "      <td>subdural</td>\n",
       "      <td>00005679d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ID  Label          Sub_type  PatientID\n",
       "361025                ID_000039fa0_any      0               any  000039fa0\n",
       "361020           ID_000039fa0_epidural      0          epidural  000039fa0\n",
       "361021   ID_000039fa0_intraparenchymal      0  intraparenchymal  000039fa0\n",
       "361022   ID_000039fa0_intraventricular      0  intraventricular  000039fa0\n",
       "361023       ID_000039fa0_subarachnoid      0      subarachnoid  000039fa0\n",
       "361024           ID_000039fa0_subdural      0          subdural  000039fa0\n",
       "3626117               ID_00005679d_any      0               any  00005679d\n",
       "3626112          ID_00005679d_epidural      0          epidural  00005679d\n",
       "3626113  ID_00005679d_intraparenchymal      0  intraparenchymal  00005679d\n",
       "3626114  ID_00005679d_intraventricular      0  intraventricular  00005679d\n",
       "3626115      ID_00005679d_subarachnoid      0      subarachnoid  00005679d\n",
       "3626116          ID_00005679d_subdural      0          subdural  00005679d"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_all = preprocess.get_all_labels('stage_1_train.csv')\n",
    "labels = labels_all[:6*N]\n",
    "labels.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels = labels['Label']\n",
    "len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels = np.array(training_labels)\n",
    "training_labels = preprocess.categorize(training_labels)\n",
    "len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = np.array(training_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 126, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 7,419,904\n",
      "Trainable params: 7,419,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(64, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/2\n",
      "450/450 [==============================] - 32s 72ms/sample - loss: 1.2234 - accuracy: 0.8378 - val_loss: 0.9409 - val_accuracy: 0.8000\n",
      "Epoch 2/2\n",
      "450/450 [==============================] - 29s 64ms/sample - loss: 0.8541 - accuracy: 0.8489 - val_loss: 0.8303 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c3d06b7f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_imgs, training_labels, epochs=2, validation_split=0.10,\n",
    "          shuffle=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative version to save the model and the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new model with the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 126, 126, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 61, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 7,419,904\n",
      "Trainable params: 7,419,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c3cde6dd8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(64, activation='softmax')\n",
    "])\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "model2.load_weights('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new model from the Json and the H5 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 126, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 7,419,904\n",
      "Trainable params: 7,419,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# from tf.keras.models import model_from_json\n",
    "# from keras.models import model_from_json\n",
    "\n",
    "# tf.keras.models.model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test/evaluate on the next N images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcm_files2 = Preprocess5.Preprocess.read_dcm_files(path_dcmFiles)[N:2*N]\n",
    "len(dcm_files2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = (128, 128)\n",
    "\n",
    "converted_testing_images = []\n",
    "# converted_training_dcmData = [] # This will be used for \n",
    "for file_name in dcm_files2:\n",
    "    dcm_data = pydicom.dcmread(file_name)\n",
    "    img = preprocess.resizing(dcm_data, desired_size)\n",
    "    converted_testing_images.append(img)\n",
    "#     converted_training_dcmData.append(temp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 pixel arrays: 187 MB\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "len(converted_testing_images)\n",
    "print(f'{N} pixel arrays: {asizeof.asizeof(converted_testing_images)//2**20} MB')\n",
    "print(f'dtype: {converted_testing_images[0].dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 normalized pixel arrays: 187 MB\n",
      "dtype: float64\n",
      "training_imgs.shape: (500, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "testing_imgs = preprocess.transform_all_pixel_arrays(converted_testing_images)\n",
    "print(f'{N} normalized pixel arrays: {asizeof.asizeof(testing_imgs)//2**20} MB')\n",
    "print(f'dtype: {testing_imgs[0].dtype}')\n",
    "testing_imgs = np.array(testing_imgs)\n",
    "print(f'training_imgs.shape: {testing_imgs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Sub_type</th>\n",
       "      <th>PatientID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1870853</th>\n",
       "      <td>ID_00318f225_any</td>\n",
       "      <td>0</td>\n",
       "      <td>any</td>\n",
       "      <td>00318f225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870848</th>\n",
       "      <td>ID_00318f225_epidural</td>\n",
       "      <td>0</td>\n",
       "      <td>epidural</td>\n",
       "      <td>00318f225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870849</th>\n",
       "      <td>ID_00318f225_intraparenchymal</td>\n",
       "      <td>0</td>\n",
       "      <td>intraparenchymal</td>\n",
       "      <td>00318f225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870850</th>\n",
       "      <td>ID_00318f225_intraventricular</td>\n",
       "      <td>0</td>\n",
       "      <td>intraventricular</td>\n",
       "      <td>00318f225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870851</th>\n",
       "      <td>ID_00318f225_subarachnoid</td>\n",
       "      <td>0</td>\n",
       "      <td>subarachnoid</td>\n",
       "      <td>00318f225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870852</th>\n",
       "      <td>ID_00318f225_subdural</td>\n",
       "      <td>0</td>\n",
       "      <td>subdural</td>\n",
       "      <td>00318f225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738613</th>\n",
       "      <td>ID_0031b5cf8_any</td>\n",
       "      <td>1</td>\n",
       "      <td>any</td>\n",
       "      <td>0031b5cf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738608</th>\n",
       "      <td>ID_0031b5cf8_epidural</td>\n",
       "      <td>0</td>\n",
       "      <td>epidural</td>\n",
       "      <td>0031b5cf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738609</th>\n",
       "      <td>ID_0031b5cf8_intraparenchymal</td>\n",
       "      <td>1</td>\n",
       "      <td>intraparenchymal</td>\n",
       "      <td>0031b5cf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738610</th>\n",
       "      <td>ID_0031b5cf8_intraventricular</td>\n",
       "      <td>0</td>\n",
       "      <td>intraventricular</td>\n",
       "      <td>0031b5cf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738611</th>\n",
       "      <td>ID_0031b5cf8_subarachnoid</td>\n",
       "      <td>0</td>\n",
       "      <td>subarachnoid</td>\n",
       "      <td>0031b5cf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738612</th>\n",
       "      <td>ID_0031b5cf8_subdural</td>\n",
       "      <td>1</td>\n",
       "      <td>subdural</td>\n",
       "      <td>0031b5cf8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ID  Label          Sub_type  PatientID\n",
       "1870853               ID_00318f225_any      0               any  00318f225\n",
       "1870848          ID_00318f225_epidural      0          epidural  00318f225\n",
       "1870849  ID_00318f225_intraparenchymal      0  intraparenchymal  00318f225\n",
       "1870850  ID_00318f225_intraventricular      0  intraventricular  00318f225\n",
       "1870851      ID_00318f225_subarachnoid      0      subarachnoid  00318f225\n",
       "1870852          ID_00318f225_subdural      0          subdural  00318f225\n",
       "1738613               ID_0031b5cf8_any      1               any  0031b5cf8\n",
       "1738608          ID_0031b5cf8_epidural      0          epidural  0031b5cf8\n",
       "1738609  ID_0031b5cf8_intraparenchymal      1  intraparenchymal  0031b5cf8\n",
       "1738610  ID_0031b5cf8_intraventricular      0  intraventricular  0031b5cf8\n",
       "1738611      ID_0031b5cf8_subarachnoid      0      subarachnoid  0031b5cf8\n",
       "1738612          ID_0031b5cf8_subdural      1          subdural  0031b5cf8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2 = labels_all[6*N: 2*6*N]\n",
    "labels2.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels = labels2['Label']\n",
    "len(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels = np.array(testing_labels)\n",
    "testing_labels = preprocess.categorize(testing_labels)\n",
    "\n",
    "testing_labels = np.array(testing_labels)\n",
    "len(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6629268136024475, Test accuracy: 0.8740000128746033\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(testing_imgs, testing_labels, verbose=0)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.37070882e-01, 1.13038674e-04, 1.45212776e-04, 8.00281018e-03,\n",
       "       1.55167378e-04, 1.93863679e-02, 5.49856981e-04, 3.10031086e-04,\n",
       "       1.25942708e-04, 1.28439385e-02, 4.27910039e-04, 3.68300622e-04,\n",
       "       9.40275568e-05, 6.17166143e-03, 1.19708908e-04, 3.19614119e-05,\n",
       "       1.68291619e-04, 2.92985421e-02, 1.92215943e-04, 7.61692238e-04,\n",
       "       2.29988975e-04, 7.03549897e-03, 7.01124445e-05, 9.55161959e-05,\n",
       "       3.41043313e-04, 5.14581567e-03, 8.19731562e-04, 3.33591779e-05,\n",
       "       3.78512079e-04, 1.21367211e-03, 1.83366690e-04, 8.62542656e-04,\n",
       "       1.04124592e-04, 4.06914800e-02, 1.20759221e-04, 3.92936025e-04,\n",
       "       6.21730796e-05, 3.31179891e-03, 7.51925691e-05, 4.68610699e-04,\n",
       "       1.51188040e-04, 4.02721763e-03, 1.07004489e-04, 3.44214961e-04,\n",
       "       7.12275578e-05, 3.05827078e-03, 1.11504269e-04, 1.18191434e-04,\n",
       "       2.61053909e-04, 5.87284006e-03, 5.97130856e-04, 7.54681387e-05,\n",
       "       1.40968186e-04, 5.23125380e-03, 5.12547267e-05, 1.71776090e-04,\n",
       "       3.33258249e-05, 2.15722757e-04, 5.51939847e-05, 2.72448397e-05,\n",
       "       2.94227444e-04, 4.47768689e-04, 4.05287778e-04, 1.56863389e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(testing_imgs, verbose=0)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping/Loading next N training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_next_training_batch(batch_index=0):\n",
    "    i = batch_index\n",
    "    \n",
    "    dcm_files2 = Preprocess5.Preprocess.read_dcm_files(path_dcmFiles)[i*N : (i+1)*N]\n",
    "    len(dcm_files2)\n",
    "\n",
    "    desired_size = (128, 128)\n",
    "\n",
    "    converted_testing_images = []\n",
    "    for file_name in dcm_files2:\n",
    "        dcm_data = pydicom.dcmread(file_name)\n",
    "        img = preprocess.resizing(dcm_data, desired_size)\n",
    "        converted_testing_images.append(img)\n",
    "\n",
    "    len(converted_testing_images)\n",
    "    print(f'{N} pixel arrays: {asizeof.asizeof(converted_testing_images)//2**20} MB')\n",
    "    print(f'dtype: {converted_testing_images[0].dtype}')\n",
    "\n",
    "    testing_imgs = preprocess.transform_all_pixel_arrays(converted_testing_images)\n",
    "    print(f'{N} normalized pixel arrays: {asizeof.asizeof(testing_imgs)//2**20} MB')\n",
    "    print(f'dtype: {testing_imgs[0].dtype}')\n",
    "    testing_imgs = np.array(testing_imgs)\n",
    "    print(f'training_imgs.shape: {testing_imgs.shape}')\n",
    "\n",
    "    labels2 = labels_all[6*N: 2*6*N]\n",
    "    labels2.head(12)\n",
    "\n",
    "    testing_labels = labels2['Label']\n",
    "    len(testing_labels)\n",
    "\n",
    "    testing_labels = np.array(testing_labels)\n",
    "    testing_labels = preprocess.categorize(testing_labels)\n",
    "\n",
    "    testing_labels = np.array(testing_labels)\n",
    "    len(testing_labels)\n",
    "\n",
    "    model.fit(training_imgs, training_labels, epochs=2, validation_split=0.10,\n",
    "              shuffle=True, batch_size=10)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_next_training_batch(batch_index=0):\n",
    "    i = batch_index\n",
    "    \n",
    "    dcm_files = Preprocess5.Preprocess.read_dcm_files(path_dcmFiles)[i*N : (i+1)*N]\n",
    "    len(dcm_files)\n",
    "\n",
    "    desired_size = (128, 128)\n",
    "\n",
    "    converted_training_images = []\n",
    "    for file_name in dcm_files:\n",
    "        dcm_data = pydicom.dcmread(file_name)\n",
    "        img = preprocess.resizing(dcm_data, desired_size)\n",
    "        converted_training_images.append(img)\n",
    "\n",
    "    len(converted_training_images)\n",
    "    print(f'{N} pixel arrays: {asizeof.asizeof(converted_training_images)//2**20} MB')\n",
    "    print(f'dtype: {converted_training_images[0].dtype}')\n",
    "\n",
    "    training_images = preprocess.transform_all_pixel_arrays(converted_training_images)\n",
    "    print(f'{N} normalized pixel arrays: {asizeof.asizeof(training_images)//2**20} MB')\n",
    "    print(f'dtype: {training_images[0].dtype}')\n",
    "    training_images = np.array(training_images)\n",
    "    print(f'training_images.shape: {training_images.shape}')\n",
    "\n",
    "    labels = labels_all[i*(6*N): (i+1)*(6*N)]\n",
    "    labels.head(12)\n",
    "\n",
    "    training_labels = labels['Label']\n",
    "    len(training_labels)\n",
    "\n",
    "    training_labels = np.array(training_labels)\n",
    "    training_labels = preprocess.categorize(training_labels)\n",
    "\n",
    "    training_labels = np.array(training_labels)\n",
    "    len(training_labels)\n",
    "\n",
    "    return training_images, training_labels\n",
    "\n",
    "def train_model(model, training_images, training_labels):\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(training_images, training_labels, epochs=2, validation_split=0.10,\n",
    "                shuffle=True, batch_size=10)\n",
    "    return model\n",
    "\n",
    "def save_model(model):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "def load_model_with_weights():\n",
    "    import tensorflow as tf\n",
    "    json_file = open('model.json', 'r')\n",
    "    model = json_file.read()\n",
    "    json_file.close()\n",
    "    model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "#     model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 pixel arrays: 37 MB\n",
      "dtype: float64\n",
      "100 normalized pixel arrays: 37 MB\n",
      "dtype: float64\n",
      "training_images.shape: (100, 128, 128, 3)\n",
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/2\n",
      "90/90 [==============================] - 5s 56ms/sample - loss: 0.6982 - accuracy: 0.8667 - val_loss: 1.2972 - val_accuracy: 0.7000\n",
      "Epoch 2/2\n",
      "90/90 [==============================] - 4s 43ms/sample - loss: 0.6198 - accuracy: 0.8667 - val_loss: 1.5563 - val_accuracy: 0.7000\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "100 pixel arrays: 37 MB\n",
      "dtype: float64\n",
      "100 normalized pixel arrays: 37 MB\n",
      "dtype: float64\n",
      "training_images.shape: (100, 128, 128, 3)\n",
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/2\n",
      "90/90 [==============================] - 7s 74ms/sample - loss: 0.7755 - accuracy: 0.8667 - val_loss: 0.0925 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "90/90 [==============================] - 6s 64ms/sample - loss: 0.6381 - accuracy: 0.8667 - val_loss: 0.1313 - val_accuracy: 1.0000\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "100 pixel arrays: 37 MB\n",
      "dtype: float64\n",
      "100 normalized pixel arrays: 37 MB\n",
      "dtype: float64\n",
      "training_images.shape: (100, 128, 128, 3)\n",
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/2\n",
      "90/90 [==============================] - 7s 77ms/sample - loss: 0.8477 - accuracy: 0.8556 - val_loss: 1.2051 - val_accuracy: 0.6000\n",
      "Epoch 2/2\n",
      "90/90 [==============================] - 7s 78ms/sample - loss: 0.6788 - accuracy: 0.8556 - val_loss: 1.3170 - val_accuracy: 0.6000\n",
      "Saved model to disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "for i in range(3):\n",
    "    training_images, training_labels = load_next_training_batch(i)\n",
    "    model = train_model(model, training_images, training_labels)\n",
    "    save_model(model)\n",
    "    model = load_model_with_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
